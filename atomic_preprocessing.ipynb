{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from visual_genome import api as vg\n",
    "import requests\n",
    "import csv\n",
    "import spacy\n",
    "import json\n",
    "import ijson\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('v4_atomic_all_agg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Events:\n",
    "      - All events start with \"PersonX\n",
    "      - All events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant small functions\n",
    "def get_number_of_entities(event):\n",
    "    if \"PersonX\" in event and \"PersonY\" in event and \"PersonZ\" in event:\n",
    "        return 3\n",
    "    if \"PersonX\" in event and \"PersonY\" in event:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "def get(verb):\n",
    "    output = []\n",
    "    cnt = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if verb in row[\"event\"]:\n",
    "            output.append(row)\n",
    "    return output\n",
    "# get(\"umbrella\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noneleted extracting relations from 108000 images\n"
     ]
    }
   ],
   "source": [
    "#Extract relations from json\n",
    "def get_relation_from_relation_dict(relation):\n",
    "    relation_str = \"\"\n",
    "    \n",
    "    #########Extract Subject#########\n",
    "    if \"name\" in relation['subject']:\n",
    "        relation_str += relation['subject']['name']\n",
    "        \n",
    "    elif \"names\" in relation['subject']:\n",
    "        if len(relation['subject']['names']) == 1:\n",
    "            relation_str += str(relation['subject']['names'][0])\n",
    "        else:\n",
    "            relation_str += \" \".join(relation['subject']['names'])\n",
    "    else:\n",
    "        relation_str += str(relation['subject'])\n",
    "    \n",
    "    #########Extract Predicate#########\n",
    "    relation_str += \" \" + str(relation[\"predicate\"]) + \" \"\n",
    "    \n",
    "    #########Extract Object#########\n",
    "    if \"name\" in relation['object']:\n",
    "        relation_str +=  relation['object']['name']\n",
    "    elif \"names\" in relation['object']:\n",
    "        if len(relation['object']['names']) == 1:\n",
    "            relation_str += str(relation['object']['names'][0])\n",
    "        else:\n",
    "            relation_str +=  \" \".join(relation['object']['names'])\n",
    "    else:\n",
    "        relation_str += str(relation['oject'])\n",
    "    \n",
    "    #########process#########\n",
    "    relation_str = relation_str.lower()\n",
    "    return relation_str\n",
    "    \n",
    "def create_relations_dict(df, images_per_json_file = 1000, clear_previous_dicts = True):\n",
    "    file = open(\"./Data/relationships.json\")\n",
    "    relations = ijson.items(file, \"item\")\n",
    "\n",
    "    if not os.path.exists(\"modelfiles\"):\n",
    "        os.mkdir(\"modelfiles\")\n",
    "        \n",
    "    if not os.path.exists(\"./modelfiles/relations_dct\"):\n",
    "        os.mkdir(\"./modelfiles/relations_dct\")\n",
    "    \n",
    "    if not os.path.exists(\"./modelfiles/data_maps/\"):\n",
    "        os.mkdir(\"./modelfiles/data_maps\")\n",
    "    \n",
    "    if clear_previous_dicts == True:\n",
    "        os.system(\"rm -rf modelfiles/relations_dct/*\")\n",
    "    \n",
    "    relations_dct = {}\n",
    "    image_id_vs_file_map = {}\n",
    "    cnt = 0\n",
    "    \n",
    "    for relation in relations:\n",
    "        cnt += 1\n",
    "        if cnt % images_per_json_file == 0:\n",
    "            sys.stdout.write(\"Completed extracting relations from \" + str(cnt) + \" images\\r\")\n",
    "            sys.stdout.flush()\n",
    "            json.dump(relations_dct, open(\"./modelfiles/relations_dct/\" + str(cnt/images_per_json_file) + \".json\" , \"w\"))\n",
    "            \n",
    "            for image_id in relations_dct.keys():\n",
    "                image_id_vs_file_map[image_id] = \"./modelfiles/relations_dct/\" + str(cnt/images_per_json_file) + \".json\"\n",
    "                \n",
    "            del relations_dct\n",
    "            relations_dct = {}\n",
    "    \n",
    "        relation_list = []\n",
    "        for r in relation[\"relationships\"]:\n",
    "            relation_list.append(get_relation_from_relation_dict(r))\n",
    "        \n",
    "        relations_dct[relation[\"image_id\"]] = relation_list\n",
    "        \n",
    "    json.dump(image_id_vs_file_map, open(\"./modelfiles/data_maps/relations_id_vs_file_map.json\", \"w\"))\n",
    "\n",
    "a = create_relations_dict(df, 1000, clear_previous_dicts=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Overlap between relations from VG and events from ATOMIC###########\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "def create_df_dict_by_events(df):\n",
    "    df_dct = {}\n",
    "    for index, row in df.iterrows():\n",
    "        df_dct[row[\"event\"]] = row\n",
    "    return df_dct\n",
    "\n",
    "def get_image_ids_by_overlap_relations_vs_events_singleFile(df_dct, relations_json_file, saveFile):\n",
    "    file = open(relations_json_file)\n",
    "    relations = ijson.items(file, \"item\")\n",
    "    image_relation_map = {}\n",
    "    \n",
    "    cnt = 1\n",
    "    for image_id in relations:\n",
    "        relation_words = set(\" \".join(relations[image_id]).split())\n",
    "        cnt += 1\n",
    "        \n",
    "        if cnt % 10 == 0:\n",
    "            print(relations_json_file + \": Done with \" + cnt + \" images\\r\")\n",
    "            sys.stdout.write(relations_json_file + \": Done with \" + cnt + \" images\\r\")\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "            \n",
    "        for event in df_dct.keys():\n",
    "            event_words = event.replace(\"PersonX\", \"\")\n",
    "            event_words = event_words.replace(\"PersonY\", \"\")\n",
    "            event_words = event_words.replace(\"PersonZ\", \"\")\n",
    "            \n",
    "            event_words = set([\"man\", \"woman\", \"person\", \"people\", \"girl\", \"boy\", \"child\", \"baby\" ].extend(event_words.split()))\n",
    "\n",
    "            intersection = event_words.intersection(relation_words)\n",
    "            \n",
    "            if len(intersection) >= THRESHOLD * len(event_words):                \n",
    "                image = vg.get_image_data(id=image_id)\n",
    "                \n",
    "                image_relation_map[event] = {\n",
    "                                                \"event_effect\": df_dct[event],\n",
    "                                                \"image_id\": image_id,\n",
    "                                                \"image_url\": image.url,\n",
    "                                                \"relations\": relations[image_id],                        \n",
    "                                            }\n",
    "    json.dump(image_relation_map, open(saveFile, \"w\"))\n",
    "    \n",
    "def get_image_ids_by_overlap_relations_vs_events(df, dir = \"./modelfiles/relations_dct/\", saveDir = \"./modelfiles/images_by_relation/\"):\n",
    "    df_dct = create_df_dict_by_events(df)\n",
    "    \n",
    "    if not os.path.exists(saveDir):\n",
    "        os.mkdir(saveDir)\n",
    "        \n",
    "    if not saveDir.endswith(\"/\"):\n",
    "        saveDir += \"/\"\n",
    "    \n",
    "    if not dir.endswith(\"/\"):\n",
    "        dir += \"/\"\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    jobs = []\n",
    "    \n",
    "    for file in os.listdir(dir):\n",
    "        job = pool.apply_async(get_image_ids_by_overlap_relations_vs_events_singleFile, (df_dct, dir + file, saveDir + file,))\n",
    "        jobs.append(job)\n",
    "        \n",
    "    for job in jobs:et\n",
    "        job.get()\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "get_image_ids_by_overlap_relations_vs_events(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 23, coco_id: -1, flickr_id: -1, width: 800, url: https://cs.stanford.edu/people/rak248/VG_100K/23.jpg\n"
     ]
    }
   ],
   "source": [
    "from visual_genome import api\n",
    "image = vg.get_image_data(id=23)\n",
    "print ( image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
